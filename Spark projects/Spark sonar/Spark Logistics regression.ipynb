{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=spark.read.csv(\"C:/Users/MVJ/Desktop/SEYNSE/Exam. codes/solar - pipelines/sonarall-data.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder,StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [] # stages in our Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= ['V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V18','V19','V20','V21','V22',\n",
    " 'V23','V24','V25','V26','V27','V28','V29','V30','V31','V32','V33','V34','V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41',\n",
    " 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58',\n",
    " 'V59', 'V60']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b3b7b671c095>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m data=dataset.select(dataset.V1.cast('float'),dataset.V2.cast('float'),dataset.V3.cast('float'),dataset.V4.cast('float'),\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV7\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV8\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV9\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV10\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV11\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV12\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV13\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV14\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV15\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV16\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV17\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV18\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV19\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV20\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "data=dataset.select(dataset.V1.cast('float'),dataset.V2.cast('float'),dataset.V3.cast('float'),dataset.V4.cast('float'),\n",
    "dataset.V5.cast('float'),dataset.V6.cast('float'),dataset.V7.cast('float'),dataset.V8.cast('float'),\n",
    "dataset.V9.cast('float'),dataset.V10.cast('float'),dataset.V11.cast('float'),dataset.V12.cast('float'),\n",
    "dataset.V13.cast('float'),dataset.V14.cast('float'),dataset.V15.cast('float'),dataset.V16.cast('float'),\n",
    "dataset.V17.cast('float'),dataset.V18.cast('float'),dataset.V19.cast('float'),dataset.V20.cast('float'),\n",
    "dataset.V21.cast('float'),dataset.V22.cast('float'),dataset.V23.cast('float'),dataset.V24.cast('float'),\n",
    "dataset.V25.cast('float'),dataset.V26.cast('float'),dataset.V27.cast('float'),dataset.V28.cast('float'),\n",
    "dataset.V29.cast('float'),dataset.V30.cast('float'),dataset.V31.cast('float'),dataset.V32.cast('float'),\n",
    "dataset.V33.cast('float'),dataset.V34.cast('float'),dataset.V35.cast('float'),dataset.V36.cast('float'),\n",
    "dataset.V37.cast('float'),dataset.V38.cast('float'),dataset.V39.cast('float'),dataset.V40.cast('float'),\n",
    "dataset.V41.cast('float'),dataset.V42.cast('float'),dataset.V43.cast('float'),dataset.V44.cast('float'),\n",
    "dataset.V45.cast('float'),dataset.V46.cast('float'),dataset.V47.cast('float'),dataset.V48.cast('float'),\n",
    "dataset.V49.cast('float'),dataset.V50.cast('float'),dataset.V51.cast('float'),dataset.V52.cast('float'),\n",
    "dataset.V53.cast('float'),dataset.V54.cast('float'),dataset.V55.cast('float'),dataset.V56.cast('float'),\n",
    "dataset.V57.cast('float'),dataset.V58.cast('float'),dataset.V59.cast('float'),dataset.V60.cast('float'),\n",
    "dataset.Class.cast('String'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_stringIdx = StringIndexer(inputCol=\"Class\", outputCol=\"label\")\n",
    "stages += [label_stringIdx]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(data)\n",
    "data2 = pipelineModel.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selectedcols = [\"label\", \"features\"]\n",
    "dataset2 = data2.select(selectedcols)\n",
    "display(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "## dividing dataset into training and test\n",
    "\n",
    "(trainingData, testData) = dataset2.randomSplit([0.7, 0.3], seed=100)\n",
    "print(trainingData.count())\n",
    "print(testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "# LogisticRegression.transform() will only use the 'features' column.\n",
    "predictions = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, prediction: double, probability: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected = predictions.select(\"label\", \"prediction\", \"probability\")\n",
    "display(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8438775510204082"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## AUC calculations\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator_forAUC = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator_forAUC.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Accuracy calculations\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator_forAccuracy= MulticlassClassificationEvaluator().setMetricName('accuracy').setLabelCol('label').setPredictionCol('prediction')\n",
    "evaluator_forAccuracy.getMetricName()\n",
    "accuracy=evaluator_forAccuracy.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn=predictions.where((predictions.label == 0) & (predictions.prediction == 0)).count()\n",
    "tp=predictions.where((predictions.label == 1) & (predictions.prediction == 1)).count()\n",
    "fp=predictions.where((predictions.label == 0) & (predictions.prediction == 1)).count()\n",
    "fn=predictions.where((predictions.label == 1) & (predictions.prediction == 0)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7777777777777778\n",
      "True Positives: 19\n",
      "True Negatives: 30\n",
      "False Positives: 5\n",
      "False Negatives: 9\n",
      "Total: 63\n",
      "Precision: 0.7916666666666666\n",
      "Recall: 0.6785714285714286\n",
      "Accuracy2: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \"+str(accuracy))\n",
    "print (\"True Positives: \"+str(tp))\n",
    "print (\"True Negatives: \"+str(tn))\n",
    "print (\"False Positives: \"+str(fp))\n",
    "print (\"False Negatives: \"+str(fn))\n",
    "print (\"Total: \"+str(predictions.count()))\n",
    "precisionn=(tp/(tp+fp))\n",
    "recalll=(tp/(tp+fn))\n",
    "acc=((tp+tn)/(tp+tn+fp+fn))\n",
    "print (\"Precision: \"+str(precisionn))\n",
    "print (\"Recall: \"+str(recalll))\n",
    "print (\"Accuracy2: \"+str(acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])  \n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])# check elastic-net regularized generalized linear models\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())\n",
    "\n",
    "cv=CrossValidator(estimator=lr,estimatorParamMaps=paramGrid,evaluator=evaluator_forAccuracy,numFolds=3)\n",
    "cvModel=cv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Intercept:  <bound method Params.explainParam of LogisticRegression_4375906eb5ca7b8014b5>\n"
     ]
    }
   ],
   "source": [
    "predictionsGrid=cvModel.transform(testData)\n",
    "evaluator_forAccuracy.evaluate(predictionsGrid)\n",
    "print('Model Intercept: ', cvModel.bestModel.explainParam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, prediction: double, probability: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c=predictions.select(\"label\", \"prediction\", \"probability\")\n",
    "display(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.79251455926873...|\n",
      "|  0.0|       0.0|[0.88260916778977...|\n",
      "|  0.0|       0.0|[0.99941025693265...|\n",
      "|  0.0|       0.0|[0.99965125729542...|\n",
      "|  0.0|       0.0|[0.99519082669886...|\n",
      "|  0.0|       0.0|[0.99651076556990...|\n",
      "|  0.0|       0.0|[0.78160932250987...|\n",
      "|  0.0|       0.0|[0.69623554562432...|\n",
      "|  0.0|       0.0|[0.84998646337528...|\n",
      "|  0.0|       1.0|[0.49439364674981...|\n",
      "|  0.0|       0.0|[0.70554107154329...|\n",
      "|  0.0|       1.0|[0.04285436175769...|\n",
      "|  0.0|       0.0|[0.66320103269790...|\n",
      "|  0.0|       0.0|[0.52603745910243...|\n",
      "|  0.0|       0.0|[0.91257633775016...|\n",
      "|  0.0|       0.0|[0.73220159643735...|\n",
      "|  0.0|       0.0|[0.99355091149573...|\n",
      "|  0.0|       0.0|[0.80909402144310...|\n",
      "|  0.0|       0.0|[0.93159655086247...|\n",
      "|  0.0|       0.0|[0.99907299350773...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnGrid=predictionsGrid.where((predictionsGrid.label == 0) & (predictionsGrid.prediction == 0)).count()\n",
    "tpGrid=predictionsGrid.where((predictionsGrid.label == 1) & (predictionsGrid.prediction == 1)).count()\n",
    "fpGrid=predictionsGrid.where((predictionsGrid.label == 0) & (predictionsGrid.prediction == 1)).count()\n",
    "fnGrid=predictionsGrid.where((predictionsGrid.label == 1) & (predictionsGrid.prediction == 0)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 19\n",
      "True Negatives: 30\n",
      "False Positives: 5\n",
      "False Negatives: 9\n",
      "Total: 63\n",
      "Precision: 0.7916666666666666\n",
      "Recall: 0.6785714285714286\n",
      "Accuracy2: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "print (\"True Positives: \"+str(tpGrid))\n",
    "print (\"True Negatives: \"+str(tnGrid))\n",
    "print (\"False Positives: \"+str(fpGrid))\n",
    "print (\"False Negatives: \"+str(fnGrid))\n",
    "print (\"Total: \"+str(predictionsGrid.count()))\n",
    "precisionn=(tpGrid/(tpGrid+fpGrid))\n",
    "recalll=(tpGrid/(tpGrid+fnGrid))\n",
    "acc=((tpGrid+tnGrid)/(tpGrid+tnGrid+fpGrid+fnGrid))\n",
    "print (\"Precision: \"+str(precisionn))\n",
    "print (\"Recall: \"+str(recalll))\n",
    "print (\"Accuracy2: \"+str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
